# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HaXH2rWYBfpZMRAdNCXx2F6AKCqEqTBP
"""

from urllib.request import urlopen
from bs4 import BeautifulSoup
from collections import Counter
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import nltk
from nltk.tokenize import word_tokenize
from transformers import AutoTokenizer
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
from collections import defaultdict
from transformers import AutoTokenizer

def get_soup(url):
    html = urlopen(url).read()
    soup = BeautifulSoup(html, features="html.parser")
    return soup

def read_text_from_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    return text

def remove_script_and_style(soup):
    for script in soup(["script", "style"]):
        script.extract()  # rip it out
    return soup

def get_clean_text(soup):
    text = soup.get_text()
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = '\n'.join(chunk for chunk in chunks if chunk)
    return text

def find_exact_string_line_numbers(text, target_name):
    lines = text.split('\n')
    line_numbers = [i + 1 for i, line in enumerate(lines) if line.strip() == target_name]
    return line_numbers

def find_all_uppercase_line_numbers(text):
    uppercase_line_numbers = [i + 1 for i, line in enumerate(text.split('\n')) if line.isupper()]
    return uppercase_line_numbers

def write_lines_between(output_file, text, start_index, end_index):
    lines_between = text.split('\n')[start_index:end_index - 1]
    output_file.write(''.join(lines_between) + '\n')

    return text
def generate_word_cloud(text, output_file_name):
    word_counts = Counter(text.split())
    common_words = word_counts.most_common()

    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')

    pdf_output_file_name = f"wordcloud_{output_file_name.split('_')[-1].split('.')[0]}.pdf"
    plt.savefig(pdf_output_file_name, format='pdf')

    plt.show()

def process_url(url, i):
    soup = get_soup(url)
    soup = remove_script_and_style(soup)
    text = get_clean_text(soup)

    target_name = "MICHAEL"
    line_numbers = find_exact_string_line_numbers(text, target_name)
    uppercase_line_numbers = find_all_uppercase_line_numbers(text)
    if i == 0:
        output_file_name = "GodFather1.txt"
    else:
        output_file_name = "GodFather2.txt"

    with open(output_file_name, 'w', encoding='utf-8') as output_file:
        for i in range(0, len(line_numbers)):
            index_in_uppercase = uppercase_line_numbers.index(line_numbers[i]) if line_numbers[i] in uppercase_line_numbers else -1
            write_lines_between(output_file, text, uppercase_line_numbers[index_in_uppercase], uppercase_line_numbers[index_in_uppercase + 1])

    # Save word cloud before removing stop words
    file_path = output_file_name
    text = read_text_from_file(file_path)
    generate_word_cloud(text, output_file_name)

    # Remove stop words and save word cloud after removing stop words
    text_without_stopwords = ' '.join([word for word in text.split() if word.lower() not in STOPWORDS])
    output_file_name = "Remove-stopword" + output_file_name
    generate_word_cloud(text_without_stopwords, output_file_name)

def compute_word_freqs(corpus, tokenizer):
    word_freqs = defaultdict(int)
    for text in corpus:
        words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
        new_words = [word for word, offset in words_with_offsets]
        for word in new_words:
            word_freqs[word] += 1
    return word_freqs

def build_alphabet(word_freqs):
    alphabet = []
    for word in word_freqs.keys():
        if word[0] not in alphabet:
            alphabet.append(word[0])
        for letter in word[1:]:
            if f"##{letter}" not in alphabet:
                alphabet.append(f"##{letter}")
    alphabet.sort()
    return alphabet

def build_splits(word_freqs):
    splits = {
        word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
        for word in word_freqs.keys()
    }
    return splits

def compute_pair_scores(splits, word_freqs):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores

def print_top_pair_scores(pair_scores, top_n=5):
    for i, (key, value) in enumerate(pair_scores.items()):
        print(f"{key}: {value}")
        if i >= top_n - 1:
            break

def find_best_pair(pair_scores):
    best_pair = max(pair_scores, key=pair_scores.get)
    max_score = pair_scores[best_pair]
    return best_pair, max_score

def merge_pair(a, b, splits, word_freqs):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits

def build_vocab(vocab_size, vocab, splits, word_freqs):
    while len(vocab) < vocab_size:
        scores = compute_pair_scores(splits, word_freqs)
        best_pair, max_score = find_best_pair(scores)
        splits = merge_pair(*best_pair, splits, word_freqs)
        new_token = (
            best_pair[0] + best_pair[1][2:]
            if best_pair[1].startswith("##")
            else best_pair[0] + best_pair[1]
        )
        vocab.append(new_token)
    return vocab

def encode_word(word, vocab):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens

def tokenize(text, tokenizer, vocab):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word, vocab) for word in pre_tokenized_text]
    return sum(encoded_words, [])

def main():
    url_list = [
        "https://www.dailyscript.com/scripts/The_Godfather.html",
        "https://www.dailyscript.com/scripts/godfather2.html",
        # Add more URLs as needed
    ]

    i = 0
    for url in url_list:
        process_url(url, i)
        i += 1

    # Read the content from the text file
    with open('GodFather1.txt', 'r') as file:
        content = file.read()
    sentences = sent_tokenize(content)

    # Initialize an empty corpus
    corpus = []

    for sentence in sentences:
        corpus.append(sentence)

    with open('GodFather2.txt', 'r') as file:
        content = file.read()

    sentences = sent_tokenize(content)

    for sentence in sentences:
        corpus.append(sentence)

    print('\n',corpus)


    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    # Compute word frequencies
    word_freqs = compute_word_freqs(corpus, tokenizer)

    # Build alphabet
    alphabet = build_alphabet(word_freqs)

    # Build splits
    splits = build_splits(word_freqs)

    # Compute pair scores
    pair_scores = compute_pair_scores(splits, word_freqs)

    # Print top pair scores
    print_top_pair_scores(pair_scores)

    # Find and print the best pair
    best_pair, max_score = find_best_pair(pair_scores)
    print("\n Best_pair:  ",best_pair, max_score,'\n')

    # Initialize vocab
    vocab = [" [PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()

    # Merge pairs to build vocab
    vocab_size = 70
    vocab = build_vocab(vocab_size, vocab, splits, word_freqs)
    print(vocab)

    # Tokenize using custom BERT-based tokenizer
    data = "Because they know that no Sicilian will refuse a request on his daughter's wedding day"
    tokens = tokenize(data, tokenizer, vocab)
    print("\n\n Word Piece tokenization:BERT-based tokenizer",tokens)
    # tokenization of sentence into words
    tokens = word_tokenize(data)

    # printing the tokens
    print("\n\n word_tokenize: ",word_tokenize(data))

if __name__ == "__main__":
    main()